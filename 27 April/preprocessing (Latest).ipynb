{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fSCoLbNGuzyP"
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIz6LUrYuzyt"
   },
   "source": [
    "# Data Input, EDA & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gwE9OANuzyv"
   },
   "source": [
    "## Individual Data Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2JBgm1EOuzyv"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Price & Volume BTC.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-324f4c266a52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprice_vol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Price & Volume BTC.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tweets.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfunding_rates_1600\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Funding Rates BTCUSDT 1600.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfunding_rates_0800\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Funding Rates BTCUSDT 0800.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfunding_rates_0000\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Funding Rates BTCUSDT 0000.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Price & Volume BTC.csv'"
     ]
    }
   ],
   "source": [
    "price_vol = pd.read_csv('Price & Volume BTC.csv')\n",
    "tweets = pd.read_csv('Tweets.csv')\n",
    "funding_rates_1600 = pd.read_csv('Funding Rates BTCUSDT 1600.csv')\n",
    "funding_rates_0800 = pd.read_csv('Funding Rates BTCUSDT 0800.csv')\n",
    "funding_rates_0000 = pd.read_csv('Funding Rates BTCUSDT 0000.csv')\n",
    "google = pd.read_csv('Google.csv')\n",
    "transactions = pd.read_csv('Transactions.csv')\n",
    "unique_addresses = pd.read_csv('Unique Addresses.csv')\n",
    "active_addresses = pd.read_csv('Active Addresses.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Vvaz18Xuzyw"
   },
   "source": [
    "## Data Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "qWcgAc7Nuzyw",
    "outputId": "d7e34167-883b-41fd-f0ef-36778ec027ad"
   },
   "outputs": [],
   "source": [
    "# get daily ave value for twitter attributes\n",
    "tweets = tweets.groupby(by=\"Time\").mean()\n",
    "tweets = tweets.reset_index()\n",
    "\n",
    "df = pd.merge(price_vol, funding_rates_0000, on=['Time'])\n",
    "df = pd.merge(df, funding_rates_0800, on=['Time'])\n",
    "df = pd.merge(df, funding_rates_1600, on=['Time'])\n",
    "df = pd.merge(df, google, on=['Time'])\n",
    "df = pd.merge(df, active_addresses, on=['Time'])\n",
    "df = pd.merge(df, unique_addresses, on=['Time'])\n",
    "df = pd.merge(df, transactions, on=['Time'])\n",
    "df = pd.merge(df, tweets, how=\"outer\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvqrC6hxuzyx",
    "outputId": "e84870cd-1aa9-48e2-8964-ef5a6fe03d52"
   },
   "outputs": [],
   "source": [
    "print(\"Type of variables: \", \"\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f50UuF_duzyy"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpBIJFNwuzyz"
   },
   "source": [
    "## Data Type Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lOBxFQYuzyz"
   },
   "outputs": [],
   "source": [
    "# Transform object type of \"Funding Rate 0000\", \"Funding Rate 0800\" and \" Funding Rate 1600\" to numerical type\n",
    "df['Funding Rate 0000'] = df['Funding Rate 0000'].map(lambda x: float('nan') if pd.isnull(x) else float(x.replace('%','')))\n",
    "df['Funding Rate 0800'] = df['Funding Rate 0800'].map(lambda x: float('nan') if pd.isnull(x) else float(x.replace('%','')))\n",
    "df['Funding Rate 1600'] = df['Funding Rate 1600'].map(lambda x: float('nan') if pd.isnull(x) else float(x.replace('%','')))\n",
    "\n",
    "df['Month'] = df['Time'].str[:7]\n",
    "df['Time2'] = df['Time'].map(lambda x: int('nan') if pd.isnull(x) else int(x.replace(\"-\", \"\")))\n",
    "df['Month'] = df['Month'].map(lambda x: int('nan') if pd.isnull(x) else int(x.replace(\"-\", \"\")))\n",
    "df['FundingRate'] = (df['Funding Rate 1600'] + df['Funding Rate 0800'] + df['Funding Rate 0000'])/3 \n",
    "df = df.sort_values(by = 'Time', ascending=True).reset_index(drop=True)\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uu9noklZuzy0"
   },
   "source": [
    "## Handle Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gf4xHBx4uzy1",
    "outputId": "df21274d-b589-434a-8afb-b58e3890fca0"
   },
   "outputs": [],
   "source": [
    "# Count missing value\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5F4J3TqVuzy1"
   },
   "source": [
    "### Monthly Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "6S6CupFGuzy2",
    "outputId": "dab024a8-108d-441b-9304-55e79e760bd6"
   },
   "outputs": [],
   "source": [
    "tweets.insert(1, \"Month\", \"NaN\")\n",
    "\n",
    "tweets['Month'] = tweets['Time'].str[:7]\n",
    "tweets['Month'] = tweets['Month'].map(lambda x: int('nan') if pd.isnull(x) else int(x.replace(\"-\", \"\")))\n",
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7JfzOLvuzy2"
   },
   "outputs": [],
   "source": [
    "tweets = tweets.groupby(by=\"Month\").mean()\n",
    "tweets = tweets.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqI4dyrwuzy3"
   },
   "outputs": [],
   "source": [
    "overall_df = pd.DataFrame(columns = ['Time', 'Month', 'Time2', 'Compound', \\\n",
    "                                 'Close', 'Volume', 'FundingRate', \\\n",
    "                             'Bitcoin_SVI', 'Cryptocurrency_SVI',  'No. of Active Addresses', \\\n",
    "                                 'No. of Unique Addresses', 'Transactions'])\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if (pd.isna(df.loc[i].at[\"Favorites\"])):\n",
    "        data = tweets[tweets['Month'] == df.loc[i].at[\"Month\"]]\n",
    "        new_row = {'Time': df.loc[i].at[\"Time\"],'Time2': df.loc[i].at[\"Time2\"], 'Month': df.loc[i].at[\"Month\"],  'Compound': data.Compound.values[0], \\\n",
    "                    \n",
    "                 'Close': df.loc[i].at[\"Close\"], 'Volume': df.loc[i].at[\"Volume\"], 'FundingRate': df.loc[i].at[\"FundingRate\"], \\\n",
    "                  'Bitcoin_SVI': df.loc[i].at[\"Bitcoin_SVI\"], \\\n",
    "                   'Cryptocurrency_SVI': df.loc[i].at[\"Cryptocurrency_SVI\"], 'No. of Active Addresses': df.loc[i].at[\"No. of Active Addresses\"], 'No. of Unique Addresses': df.loc[i].at[\"No. of Unique Addresses\"], \\\n",
    "                   'Transactions': df.loc[i].at[\"Transactions\"]}\n",
    "        overall_df = overall_df.append(new_row, ignore_index=True)\n",
    "    else:\n",
    "        new_row = {'Time': df.loc[i].at[\"Time\"], 'Time2': df.loc[i].at[\"Time2\"],'Month': df.loc[i].at[\"Month\"], 'Compound': df.loc[i].at[\"Compound\"], \\\n",
    "                  \n",
    "                 'Close': df.loc[i].at[\"Close\"],'Volume': df.loc[i].at[\"Volume\"], 'FundingRate': df.loc[i].at[\"FundingRate\"], \\\n",
    "                  'Bitcoin_SVI': df.loc[i].at[\"Bitcoin_SVI\"], \\\n",
    "                   'Cryptocurrency_SVI': df.loc[i].at[\"Cryptocurrency_SVI\"], 'No. of Active Addresses': df.loc[i].at[\"No. of Active Addresses\"], 'No. of Unique Addresses': df.loc[i].at[\"No. of Unique Addresses\"], \\\n",
    "                   'Transactions': df.loc[i].at[\"Transactions\"]}\n",
    "        overall_df = overall_df.append(new_row, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df = overall_df.drop(['Month'], axis = 1)\n",
    "overall_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "for i in range(len(overall_df)): \n",
    "    overall_df.loc[i, 'Time'] = datetime.strptime(overall_df.loc[i, 'Time'], '%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df.drop(columns=['Time2'], inplace=True)\n",
    "overall_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2                                                # proportion of dataset to be used as test set\n",
    "cv_size = 0.2                                                   # proportion of dataset to be used as cross-validation set\n",
    "N = 5                                                              # for feature at day t, we use lags from t-1, t-2, ..., t-N as features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mov_avg_std(df, col, N):\n",
    "    \"\"\"\n",
    "    Given a dataframe, get mean and std dev at timestep t using values from t-1, t-2, ..., t-N.\n",
    "    Inputs\n",
    "        df         : dataframe. Can be of any length.\n",
    "        col        : name of the column you want to calculate mean and std dev\n",
    "        N          : get mean and std dev at timestep t using values from t-1, t-2, ..., t-N\n",
    "    Outputs\n",
    "        df_out     : same as df but with additional column containing mean and std dev\n",
    "    \"\"\"\n",
    "    mean_list = df[col].rolling(window = N, min_periods=1).mean() \n",
    "    std_list = df[col].rolling(window = N, min_periods=1).std()   \n",
    "    \n",
    "    # Add one timestep to the predictions\n",
    "    mean_list = np.concatenate((np.array([np.nan]), np.array(mean_list[:-1])))\n",
    "    std_list = np.concatenate((np.array([np.nan]), np.array(std_list[:-1])))\n",
    "    \n",
    "    # Append mean_list to df\n",
    "    df_out = df.copy()\n",
    "    df_out[col + '_mean'] = mean_list\n",
    "    df_out[col + '_std'] = std_list\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax  = overall_df.plot(x = 'Time',y = 'Close', style = 'b-')\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(fontsize=6 ) \n",
    "plt.ylabel('Price of BTC (USD)')\n",
    "plt.title(\"Price of BTC From 4th March 2020 to 3rd March 2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = pd.DataFrame() \n",
    "vol['Date'] = overall_df['Time']\n",
    "vol['price'] = overall_df['Close']\n",
    "vol[\"7d_vol\"] = overall_df[\"Close\"].pct_change().rolling(7).std()\n",
    "ax  = vol.plot(x = 'Date',y = '7d_vol', style = 'b-')\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(fontsize=6 ) \n",
    "plt.ylabel('Weekly Volatility of BTC')\n",
    "plt.title(\"Weekly Volatility of BTC From 4th March 2020 to 3rd March 2022\")\n",
    "plt.savefig('Weekly_volatility.pdf')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(overall_df.isna().sum())\n",
    "overall_df1 = overall_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_cols = list(overall_df.columns)[1:]\n",
    "merging_keys = ['day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df['day'] = [x for x in list(range(len(overall_df)))]\n",
    "overall_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Correlations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(overall_df.columns)[1:-1]\n",
    "corr = overall_df[cols].astype(float).corr()\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.figure(figsize=(20,8))\n",
    "#returns an array of given shape and type as given array, with zeros\n",
    "mask = np.zeros_like(corr)\n",
    "\n",
    "# Return the indices for the upper-triangle of arr and makes it true\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(corr, cmap='RdYlGn', vmax=1.0, vmin=-1.0 , mask = mask, linewidths=1.5,annot = True)\n",
    "plt.yticks(rotation=0) \n",
    "plt.xticks(rotation=90) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_Boxplot1 = 'FundingRate'\n",
    "feature_Boxplot2 = 'No. of Active Addresses'\n",
    "feature_Boxplot3 = 'No. of Unique Addresses'\n",
    "feature_Boxplot4 = 'Transactions'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = sns.boxplot(data=overall_df[feature_Boxplot1], orient=\"h\", palette=\"Set2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = sns.boxplot(data=overall_df[feature_Boxplot2], orient=\"h\", palette=\"Set2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax3 = sns.boxplot(data=overall_df[feature_Boxplot3], orient=\"h\", palette=\"Set2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax4 = sns.boxplot(data=overall_df[feature_Boxplot4], orient=\"h\", palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winsorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(overall_df['FundingRate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before Winsorization\n",
    "overall_df['FundingRate'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding upper and lower limit\n",
    "upper_limit = overall_df['FundingRate'].quantile(0.88)\n",
    "lower_limit = overall_df['FundingRate'].quantile(0.03)\n",
    "\n",
    "print(\"Highest Allowed: \", upper_limit)\n",
    "print(\"Lowest Allowed: \", lower_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Trimming\n",
    "new_df = overall_df[(overall_df['FundingRate'] >= upper_limit) | (overall_df['FundingRate'] <= lower_limit)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(new_df['FundingRate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Capping\n",
    "overall_df['FundingRate'] = np.where(overall_df['FundingRate'] >= upper_limit,\n",
    "        upper_limit,\n",
    "        np.where(overall_df['FundingRate'] <= lower_limit,\n",
    "        lower_limit,\n",
    "        overall_df['FundingRate']))\n",
    "\n",
    "# After Winsorization\n",
    "overall_df['FundingRate'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(overall_df['FundingRate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Lagging Features (Up to 5 Days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "shift_range = [x+1 for x in range(N)]\n",
    "\n",
    "for shift in tqdm_notebook(shift_range):\n",
    "    \n",
    "    train_shift = overall_df[merging_keys + lag_cols].copy()\n",
    "    train_shift['day'] = train_shift['day'] + shift\n",
    "    foo = lambda x: '{}_lag_{}'.format(x, shift) if x in lag_cols else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "    print(train_shift)\n",
    "    overall_df = pd.merge(overall_df, train_shift, on=merging_keys, how='left') \n",
    "    \n",
    "    \n",
    "del train_shift\n",
    "\n",
    "overall_df = overall_df[N:]\n",
    "    \n",
    "overall_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df = get_mov_avg_std(overall_df, 'Close',N)\n",
    "overall_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#80-20 split - train-test \n",
    "\n",
    "train_size = 0.6 \n",
    "test_size = 0.2                                              \n",
    "val_size = 0.2                                                 \n",
    "N = 5 #For time lag (to get previous days of data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_val = int(val_size*len(overall_df))\n",
    "num_test = int(test_size*len(overall_df))\n",
    "num_train = len(overall_df) - num_val - num_test\n",
    "print(\"num_train = \" + str(num_train))\n",
    "print(\"num_val = \" + str(num_val))\n",
    "print(\"num_test = \" + str(num_test))\n",
    "\n",
    "# Split into train, cv, and test\n",
    "train = overall_df[:num_train]\n",
    "val = overall_df[num_train:num_train+num_val]\n",
    "train_val = overall_df[:num_train+num_val]\n",
    "test = overall_df[num_train+num_val:]\n",
    "print(\"train.shape = \" + str(train.shape))\n",
    "print(\"cv.shape = \" + str(val.shape))\n",
    "print(\"train_cv.shape = \" + str(train_val.shape))\n",
    "print(\"test.shape = \" + str(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time = train['Time']\n",
    "test_time = test['Time']\n",
    "val_time = val['Time']\n",
    "train_val_time = train_val['Time']\n",
    "\n",
    "train = train.drop(columns = ['Time'])\n",
    "test = test.drop(columns = ['Time'])\n",
    "val = val.drop(columns = ['Time'])\n",
    "train_val = train_val.drop(columns = ['Time'])\n",
    "\n",
    "# Select features, the target output is'Close'\n",
    "feature_pool = train.columns\n",
    "#Note : Close is a feature as well \n",
    "\n",
    "output = 'Close'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cq67OO0Puzy6"
   },
   "source": [
    "### NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.isna().sum())\n",
    "print(test.isna().sum())\n",
    "print(val.isna().sum())\n",
    "print(train_val.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_miss_filling = KNNImputer(n_neighbors=5).fit(train)\n",
    "train = pd.DataFrame(KNN_miss_filling.transform(train))\n",
    "\n",
    "KNN_miss_filling = KNNImputer(n_neighbors=5).fit(test)\n",
    "test = pd.DataFrame(KNN_miss_filling.transform(test))\n",
    "\n",
    "KNN_miss_filling = KNNImputer(n_neighbors=5).fit(train_val)\n",
    "train_val = pd.DataFrame(KNN_miss_filling.transform(train_val))\n",
    "\n",
    "KNN_miss_filling = KNNImputer(n_neighbors=5).fit(val)\n",
    "val = pd.DataFrame(KNN_miss_filling.transform(val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns = feature_pool\n",
    "test.columns = feature_pool\n",
    "train_val.columns = feature_pool\n",
    "val.columns = feature_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the train, dev and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized features:\n",
    "standardized_features = ['Compound','Volume', 'Close', 'Close_mean', 'Close_std', 'Transactions', 'Cryptocurrency_SVI', 'Bitcoin_SVI','No. of Active Addresses', 'No. of Unique Addresses']     \n",
    "for i in range(len(lag_cols)): \n",
    "    for j in range(1, N+1):\n",
    "        standardized_features.append(lag_cols[i]+\"_lag_\"+ str(j))\n",
    "non_standardized_features = list(set(train.columns)-set(standardized_features))\n",
    "non_standardized_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the scaler based on train set\n",
    "scaler = preprocessing.MinMaxScaler().fit(train[standardized_features])\n",
    "\n",
    "train_std=pd.DataFrame(scaler.fit_transform(train[standardized_features]))  # transform() return 'numpy.ndarray', not 'DataFrame' or 'Series'\n",
    "train_nstd=pd.DataFrame(train[non_standardized_features])\n",
    "\n",
    "\n",
    "train_std.columns = train_std.columns.map(lambda x: standardized_features[x])\n",
    "train_std.reset_index(drop=True, inplace=True)\n",
    "train_nstd.reset_index(drop=True, inplace=True)\n",
    "train_scaled = pd.concat([train_std,train_nstd], sort=False,axis=1)\n",
    "\n",
    "# Get the scaler based on cv set\n",
    "scaler.val = preprocessing.MinMaxScaler().fit(val[standardized_features])\n",
    "\n",
    "\n",
    "val_std=pd.DataFrame(scaler.transform(val[standardized_features]))  # transform() return 'numpy.ndarray', not 'DataFrame' or 'Series'\n",
    "val_nstd=pd.DataFrame(val[non_standardized_features])\n",
    "val_std.columns = val_std.columns.map(lambda x: standardized_features[x])\n",
    "val_std.reset_index(drop=True, inplace=True)\n",
    "val_nstd.reset_index(drop=True, inplace=True)\n",
    "val_scaled = pd.concat([val_std,val_nstd], sort=False,axis=1)\n",
    "\n",
    "\n",
    "scaler_trainval = preprocessing.MinMaxScaler().fit(train_val[standardized_features])\n",
    "\n",
    "\n",
    "train_val_std=pd.DataFrame(scaler.transform(train_val[standardized_features]))  # transform() return 'numpy.ndarray', not 'DataFrame' or 'Series'\n",
    "train_val_nstd=pd.DataFrame(train_val[non_standardized_features])\n",
    "train_val_std.columns = train_val_std.columns.map(lambda x: standardized_features[x])\n",
    "train_val_std.reset_index(drop=True, inplace=True)\n",
    "train_val_nstd.reset_index(drop=True, inplace=True)\n",
    "train_val_scaled = pd.concat([train_val_std,train_val_nstd], sort=False,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "scaler_test = preprocessing.MinMaxScaler().fit(test[standardized_features])\n",
    "\n",
    "\n",
    "test_std=pd.DataFrame(scaler.transform(test[standardized_features]))  # transform() return 'numpy.ndarray', not 'DataFrame' or 'Series'\n",
    "test_nstd=pd.DataFrame(test[non_standardized_features])\n",
    "test_std.columns = test_std.columns.map(lambda x: standardized_features[x])\n",
    "test_std.reset_index(drop=True, inplace=True)\n",
    "test_nstd.reset_index(drop=True, inplace=True)\n",
    "test_scaled = pd.concat([test_std,test_nstd], sort=False,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the order within feature pool\n",
    "feature_pool = list(train.columns)[10:]\n",
    "\n",
    "feature_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv', index = False)\n",
    "test.to_csv('test.csv', index = False)\n",
    "train_val.to_csv('train_val.csv', index = False)\n",
    "val.to_csv('val.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pool = feature_pool[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X and Y Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[feature_pool]\n",
    "y_train = train[output]\n",
    "X_val = val[feature_pool]\n",
    "y_val = val[output]\n",
    "X_train_val = train_val[feature_pool]\n",
    "y_train_val = train_val[output]\n",
    "X_test = test[feature_pool]\n",
    "y_test = test[output]\n",
    "print(\"X_train.shape = \" + str(X_train.shape))\n",
    "print(\"y_train.shape = \" + str(y_train.shape))\n",
    "print(\"X_val.shape = \" + str(X_val.shape))\n",
    "print(\"y_val.shape = \" + str(y_val.shape))\n",
    "print(\"X_train_val.shape = \" + str(X_train_val.shape))\n",
    "print(\"y_train_val.shape = \" + str(y_train_val.shape))\n",
    "print(\"X_sample.shape = \" + str(X_test.shape))\n",
    "print(\"y_sample.shape = \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = train_scaled[feature_pool]\n",
    "y_train_scaled = train_scaled['Close']\n",
    "X_val_scaled = val_scaled[feature_pool]\n",
    "y_val_scaled= val_scaled['Close']\n",
    "X_train_val_scaled = train_val_scaled[feature_pool]\n",
    "y_train_val_scaled = train_val_scaled['Close']\n",
    "X_test_scaled = test_scaled[feature_pool]\n",
    "y_test_scaled = test_scaled['Close']\n",
    "print(\"X_train_scaled.shape = \" + str(X_train_scaled.shape))\n",
    "print(\"y_train_scaled.shape = \" + str(y_train_scaled.shape))\n",
    "print(\"X_val_scaled.shape = \" + str(X_val_scaled.shape))\n",
    "print(\"y_val_scaled.shape = \" + str(y_val_scaled.shape))\n",
    "print(\"X_train_val_scaled.shape = \" + str(X_train_val_scaled.shape))\n",
    "print(\"y_train_val_scaled.shape = \" + str(y_train_val_scaled.shape))\n",
    "print(\"X_test_scaled.shape = \" + str(X_test_scaled.shape))\n",
    "print(\"y_test_scaled.shape = \" + str(y_test_scaled.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train.csv', index = False)\n",
    "y_train.to_csv('y_train.csv', index = False)\n",
    "X_val.to_csv('X_val.csv', index = False)\n",
    "y_val.to_csv('y_val.csv', index = False)\n",
    "X_train_val.to_csv('X_train_val.csv', index = False)\n",
    "y_train_val.to_csv('y_train_val.csv', index = False)\n",
    "X_test.to_csv('X_test.csv', index = False)\n",
    "y_test.to_csv('y_test.csv', index = False)\n",
    "\n",
    "X_train_scaled.to_csv('X_train_scaled.csv', index = False)\n",
    "y_train_scaled.to_csv('y_train_scaled.csv', index = False)\n",
    "X_val_scaled.to_csv('X_val_scaled.csv', index = False)\n",
    "y_val_scaled.to_csv('y_val_scaled.csv', index = False)\n",
    "\n",
    "X_train_val_scaled.to_csv('X_train_val_scaled.csv', index = False)\n",
    "y_train_val_scaled.to_csv('y_train_val_scaled.csv', index = False)\n",
    "X_test_scaled.to_csv('X_test_scaled.csv', index = False)\n",
    "y_test_scaled.to_csv('y_test_scaled.csv', index = False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_feature = pd.DataFrame(X_test_scaled[-6:-1].mean()).T\n",
    "predict_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predict_feature.to_csv(\"predict_feature.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
